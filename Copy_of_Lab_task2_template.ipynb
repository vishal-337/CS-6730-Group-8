{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vishal-337/CS-6730-Group-8/blob/main/Copy_of_Lab_task2_template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b276258d",
      "metadata": {
        "id": "b276258d"
      },
      "source": [
        "# **Task 2: Visualizing Attention and Analyzing the “Attention Sink” Phenomenon**\n",
        "\n",
        "## **Overview**\n",
        "Explores **Transformer self-attention** by capturing, visualizing, and quantifying attention maps, then simulates how **StreamLLM** leverages the *“attention sink”* to enable long-sequence generation with fixed memory.  \n",
        "The notebook adds `output_attentions=True` for introspection, builds a hook-based **Attention Catcher**, measures sink attention across layers, and contrasts standard **KV caching** with a **StreamLLM-style cache** that trims middle tokens while preserving sinks and recency.\n",
        "\n",
        "---\n",
        "\n",
        "## **Step 1: Load the Model in Investigation Mode (Cell 4)**\n",
        "\n",
        "- **Enable attention outputs:**  \n",
        "  Load the model with  \n",
        "  `AutoModelForCausalLM.from_pretrained(..., output_attentions=True)`  \n",
        "  so each forward pass returns per-layer attention tensors alongside logits.\n",
        "\n",
        "- **Tokenization setup:**  \n",
        "  Initialize the matching `AutoTokenizer` and standard generation configs used throughout experiments.\n",
        "\n",
        "---\n",
        "\n",
        "## **Step 2: Build the “Attention Catcher” Toolkit (Cell 5)**\n",
        "\n",
        "- **Global storage:**  \n",
        "  `attention_maps_storage` keeps captured attention tensors keyed by layer/module identifiers.\n",
        "\n",
        "- **Hook factory:**  \n",
        "  `get_attention_hook` returns a forward hook that extracts `attn_weights` during the pass and stores them in `attention_maps_storage`.\n",
        "\n",
        "- **Hook registrar:**  \n",
        "  `register_attention_hooks(model, layers=...)` attaches the hook to each chosen self-attention module (single layer, a subset, or all layers).\n",
        "\n",
        "- **Visualization:**  \n",
        "  `plot_attention_maps` retrieves saved maps, aggregates across heads (e.g., mean over heads), and renders **attention heatmaps** for inspection.\n",
        "\n",
        "---\n",
        "\n",
        "## **Step 3: Experiment 1 — Attention Patterns Across Inputs (Cell 6)**\n",
        "\n",
        "- **Inputs:**  \n",
        "  Define `INPUT_TEXTS` with both meaningful sentences and repetitive “dummy” strings of varying lengths.\n",
        "\n",
        "- **Hook installation:**  \n",
        "  Call `register_attention_hooks` for selected layers; clear `attention_maps_storage` before each run.\n",
        "\n",
        "- **Single forward pass:**  \n",
        "  Tokenize each input and run a forward pass to populate attention storage via hooks.\n",
        "\n",
        "- **Heatmaps:**  \n",
        "  Use `plot_attention_maps` to visualize per-layer attention, aggregated across heads, for side-by-side comparison across inputs.\n",
        "\n",
        "- **Cleanup:**  \n",
        "  Remove all hooks after the experiment to avoid extra overhead later.\n",
        "\n",
        "---\n",
        "\n",
        "## **Step 4: Experiment 2 — Quantifying the Attention Sink (Cell 7)**\n",
        "\n",
        "- **Full-depth monitoring:**  \n",
        "  Register hooks on every layer to capture a complete attention profile across the model.\n",
        "\n",
        "- **Sink metric:**  \n",
        "  For each input and layer, compute the fraction of attention mass directed to the first `SINK_TOKEN_WINDOW` tokens, averaging across heads and query positions.\n",
        "\n",
        "- **Trends by layer:**  \n",
        "  Store results and plot layer-wise curves  \n",
        "  *(x-axis: layer ID; y-axis: sink attention %)*  \n",
        "  with separate lines per input type to reveal consistent sink patterns.\n",
        "\n",
        "---\n",
        "\n",
        "## **Step 5 (Advanced): StreamLLM Simulation and Memory Advantage (Cells 8–9)**\n",
        "\n",
        "- **Positional shift attention:**  \n",
        "  `llama_pos_shift_attention_forward` modifies the attention forward path to dynamically adjust positional encodings (e.g., RoPE phases) when intermediate tokens are evicted, preserving correct relative positions among the remaining tokens.\n",
        "\n",
        "- **KV cache manager:**  \n",
        "  `streamingllm_kv` tracks and trims the KV cache by discarding middle tokens once capacity is exceeded, keeping only early “sink” tokens and the most recent tokens.\n",
        "\n",
        "- **Baseline vs. StreamLLM:**  \n",
        "  - `run_baseline_experiment`: Standard generation where KV cache grows linearly with sequence length; log memory usage over steps.  \n",
        "  - `run_streamllm_experiment`: Generation with `streamingllm_kv` trimming after each step; log memory usage for comparison.\n",
        "\n",
        "- **Analysis:**  \n",
        "  Plot both memory curves against generated tokens to show linear growth (**baseline**) vs. plateau (**StreamLLM-style trimming**), illustrating fixed-memory long-context generation.\n",
        "\n",
        "---\n",
        "\n",
        "## **Results and Takeaways**\n",
        "\n",
        "- **Memory efficiency:**  \n",
        "  StreamLLM-style KV trimming flattens memory growth, enabling sustained generation without exhausting memory.\n",
        "\n",
        "- **Output quality:**  \n",
        "  Standard full-cache generation degrades (e.g., incoherent characters) far beyond training lengths, while StreamLLM maintains more coherent outputs under extended contexts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbef0a65",
      "metadata": {
        "id": "bbef0a65"
      },
      "outputs": [],
      "source": [
        "### Cell 2: Environment Setup and Dependency Imports\n",
        "# TODO: import all required libraries (os, random, numpy, pandas, torch, transformers, etc.)\n",
        "\n",
        "RESULTS_DIR = \"./results\"\n",
        "FIGURES_DIR = \"./figures\"\n",
        "\n",
        "# TODO: create output directories if they do not exist\n",
        "# os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "# os.makedirs(FIGURES_DIR, exist_ok=True)\n",
        "\n",
        "DEVICE = None  # TODO: select torch.device based on CUDA availability\n",
        "\n",
        "# TODO: print environment diagnostics (CUDA availability, PyTorch version, etc.)\n",
        "\n",
        "def set_seed(seed: int = 42) -> None:\n",
        "    \"\"\"Seed Python, NumPy, and PyTorch RNGs for reproducible attention analysis.\"\"\"\n",
        "    ...\n",
        "\n",
        "def require_gpu(task: str) -> None:\n",
        "    \"\"\"Raise a descriptive error when a GPU-specific task cannot run.\"\"\"\n",
        "    ...\n",
        "\n",
        "# TODO: configure plotting defaults and initialise the environment\n",
        "# set_seed(42)\n",
        "# sns.set_theme(...)\n",
        "# plt.rcParams.update(...)\n",
        "# print(\"Environment initialised.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QN0aHmATG2RB",
      "metadata": {
        "id": "QN0aHmATG2RB"
      },
      "outputs": [],
      "source": [
        "# ### Cell 3: Hugging Face Login\n",
        "# from huggingface_hub import login, HfFolder\n",
        "# from getpass import getpass\n",
        "\n",
        "# # Check if a Hugging Face token is already set in the environment.\n",
        "# if not os.getenv(\"HUGGING_FACE_HUB_TOKEN\"):\n",
        "#     try:\n",
        "#         # Prompt user for Hugging Face access token if not found.\n",
        "#         hf_token = getpass(\"Please enter your Hugging Face access token: \")\n",
        "#         login(token=hf_token, add_to_git_credential=True)\n",
        "#         print(\"   Hugging Face login successful!\")\n",
        "#     except Exception as e:\n",
        "#         print(f\"Login failed: {e}. Model loading may fail later.\")\n",
        "# else:\n",
        "#     print(\"   Hugging Face token detected.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "paVt6M9LG2RB",
      "metadata": {
        "id": "paVt6M9LG2RB"
      },
      "outputs": [],
      "source": [
        "### Cell 4: Load Model and Tokenizer\n",
        "MODEL_ID = \"...\"  # TODO: primary model identifier\n",
        "FALLBACK_MODEL_ID = \"...\"  # TODO: fallback model identifier\n",
        "\n",
        "model: Optional[torch.nn.Module] = None\n",
        "tokenizer: Optional[\"AutoTokenizer\"] = None\n",
        "\n",
        "# TODO: import AutoModelForCausalLM and AutoTokenizer from transformers\n",
        "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "candidate_models = [MODEL_ID, FALLBACK_MODEL_ID]\n",
        "\n",
        "for candidate in candidate_models:\n",
        "    # TODO: attempt to load tokenizer/model with appropriate dtype and device placement\n",
        "    pass\n",
        "\n",
        "# TODO: raise an error if loading fails for all candidate models\n",
        "\n",
        "# TODO: ensure tokenizer/model pad tokens are configured\n",
        "# if tokenizer.pad_token is None:\n",
        "#     tokenizer.pad_token = tokenizer.eos_token\n",
        "# if getattr(model.config, \"pad_token_id\", None) is None:\n",
        "#     model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# TODO: place model on the desired device, switch to eval mode, and print summary stats\n",
        "# model.eval()\n",
        "# print(\"Model summary:\")\n",
        "# print(...)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3ed2d25",
      "metadata": {
        "id": "a3ed2d25"
      },
      "outputs": [],
      "source": [
        "### Cell 5: Core Functions for Attention Extraction and Visualization\n",
        "\n",
        "# Global storage for attention maps, keyed by layer name\n",
        "attention_maps_storage = {}\n",
        "\n",
        "def get_attention_hook(layer_name):\n",
        "    \"\"\"Return a forward hook function that stores attention weights for the given layer.\"\"\"\n",
        "    # TODO: capture attention tensors and store them in attention_maps_storage\n",
        "    ...\n",
        "\n",
        "def register_attention_hooks(model, layers_to_hook):\n",
        "    \"\"\"Register forward hooks on attention modules for the requested layers.\"\"\"\n",
        "    hooks = []\n",
        "    # TODO: locate attention modules (e.g., LlamaAttention) and register hooks\n",
        "    # hook_handle = attn_module.register_forward_hook(get_attention_hook(...))\n",
        "    # hooks.append(hook_handle)\n",
        "    return hooks\n",
        "\n",
        "def plot_attention_maps(attention_maps, tokens, layers_to_plot, file_prefix):\n",
        "    \"\"\"Visualise attention maps for selected layers and save the figure.\"\"\"\n",
        "    # TODO: aggregate attention across heads, configure subplots, and render heatmaps\n",
        "    ...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10ba1dbb",
      "metadata": {
        "id": "10ba1dbb"
      },
      "outputs": [],
      "source": [
        "### Cell 6: Experiment - Visualize Attention Maps for Different Inputs\n",
        "\n",
        "# --- Configurable Section ---\n",
        "INPUT_TEXTS = {\n",
        "    \"short_dummy\": \"...\",  # TODO: provide dummy prompt\n",
        "    \"short_meaningful\": \"...\",  # TODO: provide meaningful prompt\n",
        "    \"medium_dummy\": \"...\",\n",
        "    \"medium_meaningful\": \"...\",\n",
        "    \"long_dummy\": \"...\",\n",
        "    \"long_meaningful\": \"...\",\n",
        "}\n",
        "\n",
        "LAYERS_TO_VISUALIZE = [...]  # TODO: select representative layer indices\n",
        "if \"1b\" in MODEL_ID.lower():\n",
        "    LAYERS_TO_VISUALIZE = [...]  # TODO: adjust layers for smaller models\n",
        "# ---\n",
        "\n",
        "hooks = []  # TODO: register attention hooks for the selected layers\n",
        "# hooks = register_attention_hooks(model, LAYERS_TO_VISUALIZE)\n",
        "\n",
        "for name, text in INPUT_TEXTS.items():\n",
        "    print(f\"\\n--- Processing input: {name} ---\")\n",
        "    attention_maps_storage.clear()\n",
        "    # TODO: tokenize text, run the model with output_attentions=True, and collect attention maps\n",
        "    # inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=256).to(DEVICE)\n",
        "    # with torch.no_grad():\n",
        "    #     outputs = model(**inputs, output_attentions=True)\n",
        "    # tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
        "    # plot_attention_maps(attention_maps_storage, tokens, LAYERS_TO_VISUALIZE, name)\n",
        "    pass\n",
        "\n",
        "# TODO: remove hooks after visualization to avoid memory leaks\n",
        "# for handle in hooks:\n",
        "#     handle.remove()\n",
        "print(\"\\n   All attention maps for the provided inputs have been generated.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "t6SPZKujG2RD",
      "metadata": {
        "id": "t6SPZKujG2RD"
      },
      "outputs": [],
      "source": [
        "### Cell 7: Experiment - Attention Sink Phenomenon Analysis\n",
        "\n",
        "print(\"\\n--- Starting Experiment: Attention Sink Phenomenon Analysis ---\")\n",
        "# --- Configurable Section ---\n",
        "SINK_TOKEN_WINDOW = ...  # TODO: choose number of initial tokens treated as sink tokens\n",
        "# ---\n",
        "\n",
        "sink_analysis_results = []\n",
        "hooks = []  # TODO: register attention hooks across all layers\n",
        "# hooks = register_attention_hooks(model, range(model.config.num_hidden_layers))\n",
        "\n",
        "for name, text in INPUT_TEXTS.items():\n",
        "    attention_maps_storage.clear()\n",
        "    # TODO: tokenize text and run the model to capture attention maps\n",
        "    # inputs = tokenizer(text, return_tensors=\"pt\", max_length=256).to(DEVICE)\n",
        "    # with torch.no_grad():\n",
        "    #     outputs = model(**inputs, output_attentions=True)\n",
        "\n",
        "    for layer_idx in range(model.config.num_hidden_layers):\n",
        "        layer_name = f\"layer_{layer_idx}\"\n",
        "        if layer_name in attention_maps_storage:\n",
        "            # TODO: compute sink attention statistics and append to sink_analysis_results\n",
        "            # attn_map = attention_maps_storage[layer_name][0].mean(dim=0)\n",
        "            # sink_attention_strength = attn_map[SINK_TOKEN_WINDOW:, :SINK_TOKEN_WINDOW].sum().item()\n",
        "            # total_attention = attn_map[SINK_TOKEN_WINDOW:, :].sum().item()\n",
        "            # sink_percentage = (sink_attention_strength / total_attention) * 100 if total_attention > 0 else 0\n",
        "            # sink_analysis_results.append({\"Input Type\": name, \"Layer ID\": layer_idx, \"Sink Attention (%)\": sink_percentage})\n",
        "            pass\n",
        "\n",
        "# TODO: remove hooks after analysis\n",
        "# for handle in hooks:\n",
        "#     handle.remove()\n",
        "\n",
        "# TODO: convert sink_analysis_results into a DataFrame and save to disk\n",
        "# df_sink = pd.DataFrame(sink_analysis_results)\n",
        "# df_sink.to_csv(\"./results/task2_attention_sink_analysis.csv\", index=False)\n",
        "\n",
        "# TODO: plot sink attention percentage vs. layer depth for each input type\n",
        "# plt.figure(...)\n",
        "# sns.lineplot(...)\n",
        "# plt.savefig(\"./figures/task2_attention_sink_analysis.png\", dpi=300)\n",
        "\n",
        "print(\"\\n--- Attention Sink Analysis Results ---\")\n",
        "# TODO: summarise sink attention statistics and display the plot\n",
        "# print(df_sink.groupby(\"Input Type\")[\"Sink Attention (%)\"].mean().reset_index())\n",
        "# plt.show()\n",
        "\n",
        "\"\"\"\n",
        "#### Attention Sink Phenomenon Analysis\n",
        "\n",
        "**Observed Phenomena:**\n",
        "\n",
        "**Analysis:**\n",
        "\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b4e4671",
      "metadata": {
        "id": "9b4e4671"
      },
      "outputs": [],
      "source": [
        "### Cell 8: Bonus Experiment: Modify Standard Attention to StreamingLLM Attention (Task 2 Step 4)\n",
        "# TODO: import required modules for custom attention (transformers attention utilities, logging, torch.nn, types)\n",
        "# from transformers.modeling_utils import ALL_ATTENTION_FUNCTIONS\n",
        "# from transformers.models.llama.modeling_llama import LlamaAttention, rotate_half, repeat_kv\n",
        "# from transformers.utils import logging\n",
        "# import torch.nn as nn\n",
        "# import types\n",
        "\n",
        "\n",
        "def eager_attention_forward(\n",
        "    module: nn.Module,\n",
        "    query: torch.Tensor,\n",
        "    key: torch.Tensor,\n",
        "    value: torch.Tensor,\n",
        "    attention_mask: Optional[torch.Tensor],\n",
        "    scaling: float,\n",
        "    dropout: float = 0.0,\n",
        "    **kwargs,\n",
        "):\n",
        "    \"\"\"\n",
        "    Implements the standard (eager) attention forward pass.\n",
        "    \"\"\"\n",
        "    key_states = repeat_kv(key, module.num_key_value_groups)\n",
        "    value_states = repeat_kv(value, module.num_key_value_groups)\n",
        "\n",
        "    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n",
        "    if attention_mask is not None:\n",
        "        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n",
        "        attn_weights = attn_weights + causal_mask\n",
        "\n",
        "    attn_weights = nn.functional.softmax(\n",
        "        attn_weights, dim=-1, dtype=torch.float32\n",
        "    ).to(query.dtype)\n",
        "    attn_weights = nn.functional.dropout(\n",
        "        attn_weights, p=dropout, training=module.training\n",
        "    )\n",
        "    attn_output = torch.matmul(attn_weights, value_states)\n",
        "    attn_output = attn_output.transpose(1, 2).contiguous()\n",
        "\n",
        "    return attn_output, attn_weights\n",
        "\n",
        "def apply_rotary_pos_emb_single(x, cos, sin, position_ids):\n",
        "    \"\"\"\n",
        "    Applies rotary positional embedding to a single tensor.\n",
        "    \"\"\"\n",
        "    # Remove singleton dimensions for broadcasting\n",
        "    cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
        "    sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
        "    cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
        "    sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
        "    x_embed = (x * cos) + (rotate_half(x) * sin)\n",
        "    return x_embed\n",
        "\n",
        "def apply_rotary_pos_emb_q(q, cos, sin, unsqueeze_dim=1):\n",
        "    \"\"\"\n",
        "    Applies rotary positional embedding to the query tensor.\n",
        "    \"\"\"\n",
        "    cos = cos.unsqueeze(unsqueeze_dim)\n",
        "    sin = sin.unsqueeze(unsqueeze_dim)\n",
        "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
        "    return q_embed\n",
        "\n",
        "\n",
        "# TODO: refer to https://github.com/mit-han-lab/streaming-llm/blob/main/streaming_llm/pos_shift/modify_llama.py\n",
        "# modify to fit llama3 architecture\n",
        "def llama_pos_shift_attention_forward(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position=None, **kwargs):\n",
        "    \"\"\"Modified LLaMA attention forward pass with position shifting for StreamLLM.\"\"\"\n",
        "    # TODO: project QKV, update caches, apply rotary embeddings, and compute attention outputs\n",
        "    ...\n",
        "\n",
        "def enable_llama_pos_shift_attention(model):\n",
        "    \"\"\"Replace standard LlamaAttention.forward methods with the position-shifted variant.\"\"\"\n",
        "    # TODO: recursively locate LlamaAttention modules and bind llama_pos_shift_attention_forward\n",
        "    ...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e2a2eb7",
      "metadata": {
        "id": "0e2a2eb7"
      },
      "outputs": [],
      "source": [
        "### Cell 9: Bonus Experiment: Investigate StreamLLM's Impact on Long-Sequence Memory Usage\n",
        "# TODO: import tqdm for progress visualisation\n",
        "# from tqdm.auto import tqdm\n",
        "\n",
        "print(\"\\n--- Starting Bonus Experiment: Simulating StreamLLM Memory Impact ---\")\n",
        "\n",
        "# --- Configurable Section ---\n",
        "BONUS_PROMPT = \"...\"  # TODO: provide long-form prompt\n",
        "BONUS_GENERATION_LENGTH = ...  # TODO: choose number of tokens to generate\n",
        "BONUS_SAMPLING_INTERVAL = ...  # TODO: sampling interval for memory measurements\n",
        "STREAMLLM_CACHE_SIZE = ...  # TODO: number of sink tokens to retain\n",
        "STREAMLLM_RECENT_SIZE = ...  # TODO: number of most recent tokens to retain\n",
        "# ---\n",
        "\n",
        "def run_baseline_experiment(model, tokenizer, prompt, generation_length, sampling_interval, device):\n",
        "    \"\"\"Run baseline generation with the standard KV cache while logging memory usage.\"\"\"\n",
        "    # TODO: implement generation loop without cache eviction and record GPU memory\n",
        "    ...\n",
        "\n",
        "class streamingllm_kv:\n",
        "    \"\"\"Implement StreamLLM-style KV cache eviction (retain sink + recent tokens).\"\"\"\n",
        "    def __init__(self, start_size, recent_size, past_key_values):\n",
        "        # TODO: store configuration for cache trimming\n",
        "        ...\n",
        "\n",
        "    def __call__(self, kv_cache):\n",
        "        \"\"\"Trim the KV cache according to the StreamLLM policy.\"\"\"\n",
        "        # TODO: drop middle tokens while retaining sink and recent tokens\n",
        "        ...\n",
        "\n",
        "def run_streamllm_experiment(model, tokenizer, prompt, generation_length, sampling_interval, sink_size, recent_size, device):\n",
        "    \"\"\"Run generation with StreamLLM cache eviction and record memory usage.\"\"\"\n",
        "    # TODO: enable modified attention, apply streaming cache policy, and log memory\n",
        "    ...\n",
        "\n",
        "# =================================================================================\n",
        "# Main Execution Flow\n",
        "# =================================================================================\n",
        "\n",
        "# TODO: prepare chat-formatted prompt and run both baseline and StreamLLM experiments\n",
        "# messages = [...]\n",
        "# BONUS_PROMPT = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "# memory_baseline, last100words_baseline = run_baseline_experiment(...)\n",
        "# memory_streamllm, last100words_streamllm = run_streamllm_experiment(...)\n",
        "\n",
        "# TODO: collate memory usage results, save CSV artifacts, and plot comparisons\n",
        "# df_mem_compare = pd.concat([...])\n",
        "# df_mem_compare.to_csv(\"./results/task2_bonus_memory_comparison.csv\", index=False)\n",
        "# sns.lineplot(...)\n",
        "# plt.savefig(\"./figures/task2_bonus_memory_comparison.png\", dpi=300)\n",
        "\n",
        "# TODO: decode and print the final segments from each generation for qualitative comparison\n",
        "# print(tokenizer.decode(last100words_baseline))\n",
        "# print(tokenizer.decode(last100words_streamllm))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90324274",
      "metadata": {
        "id": "90324274"
      },
      "outputs": [],
      "source": [
        "### Cell 10: List all generated artifacts for Task 2\n",
        "print(\"Task 2 complete. Generated artifacts:\")\n",
        "\n",
        "# TODO: iterate over output directories and list generated files\n",
        "# if os.path.isdir(FIGURES_DIR):\n",
        "#     ...\n",
        "# if os.path.isdir(RESULTS_DIR):\n",
        "#     ...\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "spin",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}